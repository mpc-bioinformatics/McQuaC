#!/usr/bin/env python

import json
import csv
import argparse

def argparse_setup():
    parser = argparse.ArgumentParser()
    parser.add_argument("-itrfp_json", help="XIC extraction in JSON format (e.g. generated by the ThermoRawFileParser)")
    parser.add_argument("-iidentifications", help="Identifications found for the sequences of teh spiek-ins")
    parser.add_argument("-ispikeins", help="The spike-ins (in CSV)")
    parser.add_argument("-ocsv", help="The metrics in CSV")

    return parser.parse_args()
if __name__ == "__main__":

    args = argparse_setup()

    # load XICs
    with open(args.itrfp_json, "r") as trfp_in:
        xics = json.load(trfp_in)
    
    # load identifications
    identifications = dict()
    with open(args.iidentifications, "r") as ids_in:
        for l in ids_in:
            cols = l[:-1].split(",")
            identifications[cols[0]] = cols[1]
    
    # load spike_ins
    comment_to_data = dict()
    with open(args.ispikeins, "r") as in_csv_file:
        # Read spike-ins CSV and get header indicies
        csv_in = csv.reader(in_csv_file)
        header = next(csv_in)
        name_idx = header.index("name")
        seq_idx = header.index("sequence")
        mz_idx = header.index("mz")
        rt_idx = header.index("RT")

        # go line-wise through the spike-ins CSV
        for l in csv_in:
            # are there any identifications for this spike-in
            nr_ids = 0
            if str(l[seq_idx]) in identifications.keys():
                nr_ids = identifications[str(l[seq_idx])]

            # map from the comment to what we need
            comment_to_data[str(l[name_idx])] = {
                "maximum_int" : -1,
                "rt_at_maximum" : -1,
                "identifications" : nr_ids,
                "expected_rt": float(l[rt_idx]),
                "delta_rt": 0,
                "expected_mz": float(l[mz_idx])
            }

    # go through the extracted XICs and try to set the data (if XIC was successfully extracted)
    for xic in xics["Content"]:
        comment = str(xic["Meta"]["Comment"])
        if comment in comment_to_data.keys():

            rts = xic["RetentionTimes"]
            intensities = xic["Intensities"]

            max_int = max(intensities)
            max_int_rt = rts[intensities.index(max_int)] * 60

            comment_to_data[comment]["rt_at_maximum"] = max_int_rt
            comment_to_data[comment]["maximum_int"] = max_int
            comment_to_data[comment]["delta_rt"] = max_int_rt - comment_to_data[comment]["expected_rt"]
    
    # prepare output
    headers_out = []
    data_out = []
    for comment, data in comment_to_data.items():
        # "SPIKE_IDENTIFIER_MZ_XXX_RT_XXX_Maximum_Intensity"
        # "SPIKE_IDENTIFIER_MZ_XXX_RT_XXX_RT_at_Maximum_Intensity"
        # "SPIKE_IDENTIFIER_MZ_XXX_RT_XXX_FoundPSMs",
        # "SPIKE_IDENTIFIER_MZ_XXX_RT_XXX_Delta_to_expected_RT",
        spikeident = f"SPIKE_{comment}_MZ_{data['expected_mz']:.4f}_RT_{data['expected_rt']:.0f}"

        headers_out.append(f"{spikeident}_Maximum_Intensity")
        data_out.append(str(data['maximum_int']))

        headers_out.append(f"{spikeident}_RT_at_Maximum_Intensity")
        data_out.append(str(data['rt_at_maximum']))

        headers_out.append(f"{spikeident}_PSMs")
        data_out.append(str(data['identifications']))

        headers_out.append(f"{spikeident}_Delta_to_expected_RT")
        data_out.append(str(data['delta_rt']))

    # Write out the metrics
    with open(args.ocsv, "w") as output_csv:
        output_csv.write(",".join(headers_out) + "\n")
        output_csv.write(",".join(data_out))
