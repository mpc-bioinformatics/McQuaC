#!/usr/bin/env python

import json
import csv
import argparse
from typing import List, Any
import pandas as pd
import h5py
import numpy as np

def argparse_setup():
    parser = argparse.ArgumentParser()
    parser.add_argument("-itrfp_json", help="XIC extraction in JSON format (e.g. generated by the ThermoRawFileParser)")
    parser.add_argument("-iidentifications", help="Identifications found for the sequences of teh spiek-ins")
    parser.add_argument("-ispikeins", help="The spike-ins (in CSV)")
    parser.add_argument("-ohdf5", help="The metrics where to save (outputs .hdf5)")

    return parser.parse_args()



def add_entry_to_hdf5(
    f, qc_acc: str, qc_short_name: str, qc_name: str, qc_description: str, 
    value, value_shape: tuple, value_type: str, 
    unit_accession: str=None, unit_name: str=None,
    ): 
    """ Adds an entry into the hdf5 file """
    key = "|".join([qc_acc, qc_short_name])  # ACCESSION|SHORT_DESC
    if value_type in ("str", h5py.string_dtype()):
        ds = f.create_dataset(key, shape=value_shape, dtype=h5py.string_dtype(), compression="gzip")
        ds[:] = value
    else:
        f.create_dataset(key, value_shape, dtype=value_type, compression="gzip")
        f[key].write_direct(np.array(value, dtype=value_type))
    
    f[key].attrs["qc_short_name"] = qc_short_name
    f[key].attrs["qc_name"] = qc_name
    f[key].attrs["qc_description"] = qc_description
    f[key].attrs["unit_accession"] = unit_accession
    f[key].attrs["unit_name"] = unit_name



def add_table_in_hdf5(
    f, qc_acc: str, qc_short_name: str, qc_name: str, qc_description: str, 
    column_names: List[str], column_data: List[List[Any]], column_types: List[str]
    ): 
    """Adds a table in groups"""

    key = "|".join([qc_acc, qc_short_name])  # ACCESSION|SHORT_DESC
    table_group = f.create_group(key)
    table_group.attrs["qc_short_name"] = qc_short_name
    table_group.attrs["qc_name"] = qc_name
    table_group.attrs["qc_description"] = qc_description
    table_group.attrs["column_order"] = "|".join(column_names)
    
    for n, d, t in zip (column_names, column_data, column_types): 
        if t in ("str", h5py.string_dtype()):
            ds = table_group.create_dataset(n, shape=len(d), dtype=h5py.string_dtype(), compression="gzip")
            ds[:] = d
        else:
            table_group.create_dataset(n, (len(d),), dtype=t, compression="gzip")
            table_group[n].write_direct(np.array(d, dtype=t))
    
if __name__ == "__main__":
    args = argparse_setup()

    # load XICs
    with open(args.itrfp_json, "r") as trfp_in:
        xics = json.load(trfp_in)
    
    # load identifications
    identifications = dict()
    with open(args.iidentifications, "r") as ids_in:
        for l in ids_in:
            cols = l[:-1].split(",")
            identifications[cols[0]] = cols[1]
    
    # load spike_ins
    comment_to_data = dict()
    with open(args.ispikeins, "r") as in_csv_file:
        # Read spike-ins CSV and get header indicies
        csv_in = csv.reader(in_csv_file)
        header = next(csv_in)
        name_idx = header.index("name")
        seq_idx = header.index("sequence")
        mz_idx = header.index("mz")
        rt_idx = header.index("RT")

        # go line-wise through the spike-ins CSV
        for l in csv_in:
            # are there any identifications for this spike-in
            nr_ids = 0
            if str(l[seq_idx]) in identifications.keys():
                nr_ids = identifications[str(l[seq_idx])]

            # map from the comment to what we need
            comment_to_data[str(l[name_idx])] = {
                "maximum_int" : -1,
                "rt_at_maximum" : -1,
                "identifications" : nr_ids,
                "expected_rt": float(l[rt_idx]),
                "delta_rt": 0,
                "expected_mz": float(l[mz_idx])
            }

    # go through the extracted XICs and try to set the data (if XIC was successfully extracted)
    for xic in xics["Content"]:
        comment = str(xic["Meta"]["Comment"])
        if comment in comment_to_data.keys():

            rts = xic["RetentionTimes"]
            intensities = xic["Intensities"]

            # default value if no intensities are found
            max_int = 0
            max_int_rt = -1

            if intensities is not None and len(intensities) > 0:
                max_int = max(intensities)
                max_int_rt = rts[intensities.index(max_int)] * 60

            comment_to_data[comment]["rt_at_maximum"] = max_int_rt
            comment_to_data[comment]["maximum_int"] = max_int
            comment_to_data[comment]["delta_rt"] = max_int_rt - comment_to_data[comment]["expected_rt"]

    # prepare output
    with h5py.File(args.ohdf5, 'w') as out_h5:

        column_names = [
            "Spike-in",
            "Maximum_Intensity",
            "RT_at_Maximum_Intensity",
            "PSMs",
            "Delta_to_expected_RT"
        ]
        column_data: List[List[Any]] = [[] for _ in column_names]
        column_names_types = ["str", "float", "float", "int", "float"]

        for comment, data in comment_to_data.items():
            spikeident = f"SPIKE_{comment}_MZ_{data['expected_mz']:.4f}_RT_{data['expected_rt']:.0f}"

            column_data[0].append(spikeident)

            column_data[1].append(data['maximum_int'])

            column_data[2].append(data['rt_at_maximum'])

            column_data[3].append(data['identifications'])

            column_data[4].append(data['delta_rt'])


        add_table_in_hdf5(
            out_h5,
            "LOCAL:30",
            "spike_in_metrics",
            "Spike-in metrics",
            (
                "Table of various spike-ins metrics like, "
                "max. intensity, RT at max. intensity, PSMs, and delta to expected RT."
            ),
            column_names,
            column_data,
            column_names_types,
        )
